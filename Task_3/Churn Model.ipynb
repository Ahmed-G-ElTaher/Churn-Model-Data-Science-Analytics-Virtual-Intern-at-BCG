{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "1. Import packages\n",
    "2. Load data\n",
    "3. Feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.precision', 10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./clean_data_after_eda.csv')\n",
    "df[\"date_activ\"] = pd.to_datetime(df[\"date_activ\"], format='%Y-%m-%d')\n",
    "df[\"date_end\"] = pd.to_datetime(df[\"date_end\"], format='%Y-%m-%d')\n",
    "df[\"date_modif_prod\"] = pd.to_datetime(df[\"date_modif_prod\"], format='%Y-%m-%d')\n",
    "df[\"date_renewal\"] = pd.to_datetime(df[\"date_renewal\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature engineering\n",
    "\n",
    "### Difference between off-peak prices in December and preceding January\n",
    "\n",
    "Below is the code created by your colleague to calculate the feature described above. Use this code to re-create this feature and then think about ways to build on this feature to create features with a higher predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = pd.read_csv('price_data.csv')\n",
    "price_df[\"price_date\"] = pd.to_datetime(price_df[\"price_date\"], format='%Y-%m-%d')\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group off-peak prices by companies and month\n",
    "monthly_price_by_id = price_df.groupby(['id', 'price_date']).agg({'price_off_peak_var': 'mean', 'price_off_peak_fix': 'mean'}).reset_index()\n",
    "\n",
    "# Get january and december prices\n",
    "jan_prices = monthly_price_by_id.groupby('id').first().reset_index()\n",
    "dec_prices = monthly_price_by_id.groupby('id').last().reset_index()\n",
    "\n",
    "# Calculate the difference\n",
    "diff = pd.merge(dec_prices.rename(columns={'price_off_peak_var': 'dec_1', 'price_off_peak_fix': 'dec_2'}), jan_prices.drop(columns='price_date'), on='id')\n",
    "diff['offpeak_diff_dec_january_energy'] = diff['dec_1'] - diff['price_off_peak_var']\n",
    "diff['offpeak_diff_dec_january_power'] = diff['dec_2'] - diff['price_off_peak_fix']\n",
    "diff = diff[['id', 'offpeak_diff_dec_january_energy','offpeak_diff_dec_january_power']]\n",
    "diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(df, diff, how=\"left\", on=\"id\")\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ______________________________________________________________________\n",
    "# ======================================================================\n",
    "# ______________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "data['channel_sales'] = le.fit_transform(data['channel_sales'])\n",
    "data['has_gas'] = le.fit_transform(data['has_gas'])\n",
    "data['origin_up'] = le.fit_transform(data['origin_up'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['join_days'] = data['date_end']  - data['date_activ']\n",
    "data['join_days'] = data['join_days'].astype(str).str.split(pat=' ').str[0].astype(int) \n",
    "\n",
    "data['diff_modfi_act_days'] = data['date_modif_prod']  - data['date_activ']\n",
    "data['diff_modfi_act_days'] = data['diff_modfi_act_days'].astype(str).str.split(pat=' ').str[0].astype(int) \n",
    "\n",
    "data['diff_renw_act_days'] = data['date_renewal']  - data['date_activ']\n",
    "data['diff_renw_act_days'] = data['diff_renw_act_days'].astype(str).str.split(pat=' ').str[0].astype(int) \n",
    "\n",
    "data['diff_end_renw_year'] = data['date_end']  - data['date_renewal']\n",
    "data['diff_end_renw_year'] = data['diff_end_renw_year'].astype(str).str.split(pat=' ').str[0].astype(int) / 365\n",
    "\n",
    "data['diff_end_modif_days'] = data['date_end']  - data['date_modif_prod']\n",
    "data['diff_end_modif_days'] = data['diff_end_modif_days'].astype(str).str.split(pat=' ').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train , test = train_test_split(data.drop(columns=['date_end','date_activ','date_modif_prod','date_renewal'])\n",
    "                                 ,random_state=104, test_size=0.2, shuffle=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.drop(columns=['id','churn'])\n",
    "y = train.churn\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=104, test_size=0.15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Start with a Dummy Model (np.rand) - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = y_test.shape[0]\n",
    "rand_y_test = np.random.randint(2,size= m)\n",
    "rand_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_accuarcy = acc(y_test,rand_y_test)\n",
    "random_accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_f1 = f1(y_test,rand_y_test)\n",
    "random_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Simple Model (linear)\n",
    "- Linear model\n",
    "- calculate score\n",
    "- calculate feature imporance\n",
    "- Simple model with top 10/20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred), f1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame({\"features\":list(x_train.columns), \"coef\":list(model.coef_[0])}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp.plot(x=\"features\", y=\"coef\", kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp.sort_values(\"coef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_col = feature_imp[np.abs(feature_imp[\"coef\"])>0.0000155863][\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[list(top_15_col)]\n",
    "y = train.churn\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=104, test_size=0.15, shuffle=True)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred), f1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Simple Model with Balanced Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First : Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "print('Before UpSampling, the shape of train_X: {}'.format(x_train.shape))\n",
    "print('Before UpSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n",
    "\n",
    "print(\"Before UpSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique\n",
    "x_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())\n",
    "\n",
    "\n",
    "print(\"After UpSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y_train_res==0)))\n",
    "\n",
    "\n",
    "\n",
    "print('After UpSampling, the shape of train_X: {}'.format(x_train_res.shape))\n",
    "print('After UpSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_res,y_train_res)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second : Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After UpSampling, counts of label '1': 965\n",
      "After UpSampling, counts of label '0': 965 \n",
      "\n",
      "After UpSampling, the shape of train_X: (1930, 15)\n",
      "After UpSampling, the shape of train_y: (1930,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "x_rus, y_rus = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "\n",
    "print(\"After UpSampling, counts of label '1': {}\".format(sum(y_rus==1)))\n",
    "print(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y_rus==0)))\n",
    "\n",
    "\n",
    "\n",
    "print('After UpSampling, the shape of train_X: {}'.format(x_rus.shape))\n",
    "print('After UpSampling, the shape of train_y: {} \\n'.format(y_rus.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5681688533941814 0.21554404145077719\n",
      "[[892 694]\n",
      " [ 63 104]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.56      0.70      1586\n",
      "           1       0.13      0.62      0.22       167\n",
      "\n",
      "    accuracy                           0.57      1753\n",
      "   macro avg       0.53      0.59      0.46      1753\n",
      "weighted avg       0.86      0.57      0.66      1753\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_res,y_train_res)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Complex and Explainable Model (Tree Based)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829435253850542 0.09667673716012085\n",
      "[[1438  148]\n",
      " [ 151   16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91      1586\n",
      "           1       0.10      0.10      0.10       167\n",
      "\n",
      "    accuracy                           0.83      1753\n",
      "   macro avg       0.50      0.50      0.50      1753\n",
      "weighted avg       0.83      0.83      0.83      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier With original data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7974900171135196 0.1362530413625304\n",
      "[[1370  216]\n",
      " [ 139   28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89      1586\n",
      "           1       0.11      0.17      0.14       167\n",
      "\n",
      "    accuracy                           0.80      1753\n",
      "   macro avg       0.51      0.52      0.51      1753\n",
      "weighted avg       0.83      0.80      0.81      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier With Upsampling data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_res,y_train_res)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5145464917284654 0.17618586640851888\n",
      "[[811 775]\n",
      " [ 76  91]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.51      0.66      1586\n",
      "           1       0.11      0.54      0.18       167\n",
      "\n",
      "    accuracy                           0.51      1753\n",
      "   macro avg       0.51      0.53      0.42      1753\n",
      "weighted avg       0.84      0.51      0.61      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier With Downsampling data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_rus,y_rus)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9070165430690246 0.0790960451977401\n",
      "[[1583    3]\n",
      " [ 160    7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      1586\n",
      "           1       0.70      0.04      0.08       167\n",
      "\n",
      "    accuracy                           0.91      1753\n",
      "   macro avg       0.80      0.52      0.52      1753\n",
      "weighted avg       0.89      0.91      0.87      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier With Original data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87906446092413 0.09401709401709403\n",
      "[[1530   56]\n",
      " [ 156   11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      1586\n",
      "           1       0.16      0.07      0.09       167\n",
      "\n",
      "    accuracy                           0.88      1753\n",
      "   macro avg       0.54      0.52      0.51      1753\n",
      "weighted avg       0.84      0.88      0.86      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier With Upsampling data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_res,y_train_res)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5670279520821448 0.18996798292422626\n",
      "[[905 681]\n",
      " [ 78  89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.57      0.70      1586\n",
      "           1       0.12      0.53      0.19       167\n",
      "\n",
      "    accuracy                           0.57      1753\n",
      "   macro avg       0.52      0.55      0.45      1753\n",
      "weighted avg       0.84      0.57      0.66      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier With Downsampling data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_rus,y_rus)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Deeper Model (XGBoost,catboost)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8996006845407872 0.043478260869565216\n",
      "[[1573   13]\n",
      " [ 163    4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95      1586\n",
      "           1       0.24      0.02      0.04       167\n",
      "\n",
      "    accuracy                           0.90      1753\n",
      "   macro avg       0.57      0.51      0.50      1753\n",
      "weighted avg       0.84      0.90      0.86      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier With original data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8796349115801483 0.11715481171548119\n",
      "[[1528   58]\n",
      " [ 153   14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      1586\n",
      "           1       0.19      0.08      0.12       167\n",
      "\n",
      "    accuracy                           0.88      1753\n",
      "   macro avg       0.55      0.52      0.53      1753\n",
      "weighted avg       0.84      0.88      0.86      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier With Upsampling data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train_res,y_train_res)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5430690245293782 0.1917255297679112\n",
      "[[857 729]\n",
      " [ 72  95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.54      0.68      1586\n",
      "           1       0.12      0.57      0.19       167\n",
      "\n",
      "    accuracy                           0.54      1753\n",
      "   macro avg       0.52      0.55      0.44      1753\n",
      "weighted avg       0.85      0.54      0.63      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier With Downsampling data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_rus,y_rus)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second : CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9053051911009697 0.03488372093023255\n",
      "[[1584    2]\n",
      " [ 164    3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      1586\n",
      "           1       0.60      0.02      0.03       167\n",
      "\n",
      "    accuracy                           0.91      1753\n",
      "   macro avg       0.75      0.51      0.49      1753\n",
      "weighted avg       0.88      0.91      0.86      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CatBoostClassifier With original data\n",
    "model = CatBoostClassifier(verbose=0)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8876212207644039 0.10859728506787329\n",
      "[[1544   42]\n",
      " [ 155   12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      1586\n",
      "           1       0.22      0.07      0.11       167\n",
      "\n",
      "    accuracy                           0.89      1753\n",
      "   macro avg       0.57      0.52      0.52      1753\n",
      "weighted avg       0.84      0.89      0.86      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CatBoostClassifier With Upsampling data\n",
    "model = CatBoostClassifier(verbose=0)\n",
    "model.fit(x_train_res,y_train_res)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573873359954364 0.19763694951664876\n",
      "[[914 672]\n",
      " [ 75  92]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.58      0.71      1586\n",
      "           1       0.12      0.55      0.20       167\n",
      "\n",
      "    accuracy                           0.57      1753\n",
      "   macro avg       0.52      0.56      0.45      1753\n",
      "weighted avg       0.85      0.57      0.66      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CatBoostClassifier With Downsampling data\n",
    "model = CatBoostClassifier(verbose=0)\n",
    "model.fit(x_rus,y_rus)\n",
    "y_pred = model.predict(x_test)\n",
    "print(acc(y_test,y_pred),\n",
    "       f1(y_test, y_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Best Model is : CatBoostClassifier With Downsampling data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "152bf6e7dc8ee53edb5af21dc1a8faeab7f134840808a94079ed98d91ece7e0c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
